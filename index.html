<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Causal inference algorithms for learning in neural networks </title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">
		<link rel="stylesheet" href="cust_black.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Mathjax for math typesetting -->
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({TeX: {extensions: ["color.js"]}});
		</script>
		
		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<section data-background="assets/brainbow2.jpg">
					<h1>Causal inference algorithms for learning in neural networks </h1>
				<hr>
				<p style="text-align: center; font-size: larger; text-shadow: 0px 0px 0px #0000ff;">Ben Lansdell, Bioengineering UPenn<br><br>Rutgers-Newark Mathematics & Computer Science. February 5th 2020
				<aside class="notes">
				  <span style="color: red">
				  </span> •		  
				  • <span style="color: green"></span>
				</aside>
				  </section>
		  
		  <!-- ----------------------------------------------------------------------- -->
		  <!-- The overall introduction -->
		  <!-- ---------------------------------------------------------------------- -->

	  <section data-background="assets/brainbow2.jpg">
		<h2>Two complementary tasks to understand intelligence</h2>
		<div>
		<div id = "left">
			1. Build artificial systems
			<img src="assets/alphago.jpeg" width="75%">
		</div>
		<div id = "right">
		2. Study human intelligence
		<img src="assets/Brain-PNG-Image.png" width="90%">
		</div>
		</div>
		<div style="clear: both">
		<p style="text-align: center"><br>Learning is central to both human and artificial intelligence</p>
		<br>
		<p style="text-align: center" class="fragment">$\Rightarrow$ Advances in each domain can inspire the other</p>
		</div>
		<aside class="notes">
		<span style="color: red"></span> •
			But nonetheless, we can still conclude that learning is central to both human and artificial intelligence. 

			This convergence of interests means that advances in one domain can inspired progress in the other. AI, or more generally machine learning, may inspire models of learning in neuroscience, and vice versa.
		• <span style="color: green"></span>
	  </aside>	</section>

<!-- Slide about causality-->
<section data-background="assets/brainbow2.jpg">
	<h2>Machine learning, neuroscience, and causality</h2>
	<em>Causation</em> relates to a number of challenges in both machine learning and neuroscience
	<div>
	<div id = "left">
		<br>
		<img src="assets/chocolate.png" width="93%">
		<p class="rcred">Messerli, N Engl J Med 2012</p>
	</div>
	<div id = "right">
		<br>
		<p style="text-align: left">In ML:</p>
		<ul>
			<li>Causal models are more robust to changes in environment/distribution: better transfer, generalization</li>
			<li>Fairness: strong associations are not causal, and may be unfair/biased/prejudiced</li>
			<li>Safety: observational data may not say what happens when we act/intervene/change distributions</li>
		</ul>
	</div>
	</div>
	<aside class="notes">
	<span style="color: red"></span> •
		So, given this overlap, when we think about challenges in both machine learning, and neuroscience. In this talk I wish to make the point that a number of these in fact relate to causation. 

		[Explain chocolate example, and how it doesn't say anything about how we might use this relationship to control or change the system to some desired outcome. Yet most recent advances in AI and ML are based on building better predicted models. ]

		This has a number of important consequences:
		* causal models are more robust to changes in the environment. This means that we should expect learning causal models to be more generalizable to other environments, distributions, etc, and this means it doesn't have to start from scratch in every new task you're interested in -- causal models can be more data efficient
		* strong associations need not be causal, and making policy decisions based on associations may be biased, prejudiced, or unfair -- 
		* another way to phrase this is that observational data may not say what happens when we change the system, and thus there are no safey guarantees about what happens when you do change things. 

		These are some of the key challenges facing machine learning today. 
	• <span style="color: green"></span>
  </aside>	</section>

  <section data-background="assets/brainbow2.jpg">
	<h2>Machine learning, neuroscience, and causality</h2>
	<em>Causation</em> relates to a number of challenges in both machine learning and neuroscience
	<div>
	<div id = "left">
		<br>
		<img src="assets/CorticalFOV.svg" style="border: 0px" width="75%">
	</div>
	<div id = "right">
		<br>
		<p style="text-align: left">In neuroscience:</p>
		<ul>
			<li>Data analysis: 
			<ul>
				<li> neural datasets generally hugely undersampled &ndash; confounding, interpretation
				<li> increased ability to perturb specific circuits
			</ul>
			<li>Efficient learning, transfer, generalization</li>
			<li>Causal learning</li>
		</ul>
	</div>
	</div>
	<aside class="notes">
	<span style="color: red"></span> •
	In neuroscience, we generally seek a mechanistic, or causal model, of a system. Yet, only recently have we begun to have good enough experimental tools to do that. Even with current methods, it's generally the case that neural data is hugely undersample, which can introduce confounding, and this makes intepreting relationships in neural data difficult. 

	More generally, when we think about the tasks that neuronal networks perform, some of the same issues relate: how do we learn transferable models of the world, and how does this relate to learning causal relationships between our own actions and the world, and between other objects in the world?
	• <span style="color: green"></span>
  </aside>	</section>

  <section data-background="assets/brainbow2.jpg">
	<h2>Machine learning, neuroscience, and causality</h2>
	<em>Causation</em> relates to a number of challenges in both machine learning and neuroscience
	<div>
	<div id = "left">
		<br>
		<img src="assets/chocolate.png" width="93%">
	</div>
	<div id = "right">
		<br>
		<img src="assets/CorticalFOV.svg" style="border: 0px" width="75%">
	</div>
	</div>
	<div style="clear: both">
		<br>
		Claim: progress in both machine learning and neuroscience can come from explicitly casting problems as causal learning problems
	</div>
	<aside class="notes">
	<span style="color: red"></span> •
		* The claim I will present in this talk is that progress in both machine learning and neuroscience can come from explicitly casting problems as causal learning problems, and applying existing causal learning methods to problems in machine learning and neuro 
	• <span style="color: green"></span>
  </aside>	</section>

  <section data-background="assets/brainbow2.jpg">
	<h2>Outline</h2>
	<ol>
		<li class="fragment highlight-green"> The neuronal credit assignment problem as causal inference
		<li> Learning to solve the credit assignment problem
		<li>Causal learning and decision making</li>
	</ol>
		<aside class="notes">
	<span style="color: red"></span> •
	* For the bulk of this talk, the aim is to see how that plays out in one particular example in detail, in particular in a problem called the credit assignment problem
	• <span style="color: green"></span>
  </aside>	</section>	  


  		  <!-- --------------------------------------------- -->
		  <!-- Credit assignment problem as causal inference -->
		  <!-- --------------------------------------------- -->
		  <section data-background-color="#ffffff">
			<h2>Learning &ndash; improving performance over time</h2>
			<div>
			<div>
			  <img src="assets/losslandscape.svg" width="50%">
			</div>
		  </div>

		  Find parameters $\theta$ that minimize a loss/maximize a reward function, $R$

		  <aside class="notes">
			<span style="color: red"></span> •
			Ok, so what do I mean by the credit assignment problem?
			
			This problem occurs in learning. The important thing to keep in when studying learning, either in the brain or in machines, is that learning is a normative activity -- there is a sense of better or worse, of improvement. So that, in machine learning we formalize this as a loss function, or as expected future reward in RL, and study how the system can or should act to minimize this loss function.

			We'll use the framework to study this question. We'll have a loss function R. We'll think of training a deep neural network with N hidden layers, and we'll aim to find the network parameters W that minimize the expected loss. 
			• <span style="color: green"></span>
		  </aside>	</section>

			  <section data-background-color="#ffffff">
				  <h2>Learning in the brain</h2>
				  <div>
				  <div style="text-size: 40pt">
					<br><br><br><br><br>
					<img src="assets/weightupdates.gif" width="50%">
					<br><br><br><br><br>
				  </div>
				</div>
				What are the synaptic update rules used by neurons that provide efficient and flexible learning?
				<aside class="notes">
				  <span style="color: red"></span> •
				  • <span style="color: green"></span>
				</aside>	</section>
		  
			  <section data-background-color="#ffffff">
				<h2>The neuronal credit assignment problem</h2>
				<p>To learn, a neuron must know its effect on the loss function</p><br>
				<img src="assets/spiketrain.svg" width="80%">
				<p><br>In spiking neural networks, this means something like:</p>
				<ul>
					<li> If, for a given input, a spike <b>increases</b> the loss, the weights leading to that spike should <b>decrease</b>
					<li> If, for a given input, a spike <b>decreases</b> the loss, the weights leading to that spike should <b>increase</b>
				</ul>
				</aside></section>
		  
		  <!-- ---------------------------------------------------------------------- -->
		  <!-- ---------------------------------------------------------------------- -->
		  <!-- ---------------------------------------------------------------------- -->
		  
			  <section data-background-color="#ffffff">
				<h2>The problem: noise correlations and confounding</h2>
				  <img src="assets/correlation_a.png" width="80%">
				  <p class="fragment" style="text-align: center">$\Rightarrow$ Viewing learning as a <em>causal inference</em> problem may provide insight</p>
				  <aside class="notes">
				  <span style="color: red"></span> •
					So we can think about the first of these problem as follows. If we take a neuron's perspective, neuron 1's. Then in general it is the case that, even for a fixed stimulus, there are correlations between neuron 1 and a neighboring neuron, neuron 2. These correlations can act as confounders. Say neuron 1 observes a negative relationship between its activity and a reward signal. Just based on observing reward, and even neuron 2's activity, it cannot determine if it has any direct effect on the reward signal? These two causal graphs produce the same joint probability distribution. 
					
					If a neuron want to change its synaptic weights in this setting, how should it go about it? 

					Viewing learning as a causal inference problem can provide insight. 

				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
			  <section data-background-color="#ffffff">
				  <h2>Causality</h2>
				  <img src="assets/correlation_a.png" width="80%">
				  <ul>
					<li> Defined in terms of counterfactuals or interventions
					<li> The causal effect: $\beta = \mathbb{E}(R|H\leftarrow 1) - \mathbb{E}(R|H\leftarrow 0)$
					<li> How can we predict the causal effect from observation?
				</ul>	    
			  <aside class="notes">
				  <span style="color: red"></span> •
					To keep going I need to make the idea of a causal relationship a bit more precise. What do I mean by a causal relationship? This is, in stats at least, normally defined in terms of interventions of counter factuals. By intervention we mean something that effects the value of one variable. In essence, we force a given variable to take a given value, and erase any relationship it may have previously had with its potential causes. 

					So here, we force neuron 1 to take on a given value, regardless of what neuron 2 is doing. We can see this actually allows us to now distinguish between the two graphs. We can say through interventions if neuron 1 has a direct effect on reward or not. 

					Often we're itnerested in a quantity called the causal effect. E.g. if this is a drug trial, this is expected outcome given treatment or control conditions. 

					A key question in the field is, can we determine the causal effect from observational data (that is, without actually doing the experiments to intervene)?

					If you follow these two slides you get the basis of Pearl's framework for causal inference. And two important things: observational data may not be enough to tell two causal graphs apart, but interventions do make them identifiable. 

				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
			  <section data-background-color="#ffffff">
				<h2>Causality</h2>
				<img src="assets/intervention.svg" width="80%">
				<ul>
				  <li> Defined in terms of counterfactuals or interventions
				  <li> The causal effect: $\beta = \mathbb{E}(R|H\leftarrow 1) - \mathbb{E}(R|H\leftarrow 0)$
				  <li> How can we predict the causal effect from observation?
			  </ul>	    
			<aside class="notes">
				<span style="color: red"></span> •
				• <span style="color: green"></span>
			  </aside>
			</section>
		  
			  <section data-background-color="#ffffff">
				  <h2>Credit assignment as causal inference</h2>
				  <p>What is a neuron's causal effect on reward, and so how should it change to improve performance?
						  $$
						  \beta_i = \mathbb{E}(R|  H_i \leftarrow 1) - \mathbb{E}(R| H_i \leftarrow 0)
						  $$
						</p>
				  <p class="fragment">$\Rightarrow$ How can a neuron perform causal inference?</p>
				  <aside class="notes">
				  <span style="color: red"></span> •
				  So, given these ideas we can rephrase our question about credit assignment again. We can rephase our problem into a network wanting to know which actions cause reward. And, what is a neuron's causal effect on reward, and so how should it change to improve? 

				  Given this interpretation, we can see that methods like those based on REINFORCE address this problem by adding randomization to a neuron's output. That is, you can think of this as a type of RCT approach to doing this causal inference. 

				  The question is, can a neuron solve this causal inference problem without randomizing? 
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
			  <section data-background-color="#ffffff">
				<h2>Credit assignment as causal inference</h2>
				<p>One solution: Randomization</p>

				<p>If independent (unconfounded) noise is added to the system, this can be correlated with reward for an estimate of loss gradients</p>

				<p>In fact, the REINFORCE algorithm correlates reward with independent pertubations in activity, $\xi$:
					$$
					\mathbb{E}( R\xi ) \approx \sigma^2 \frac{\delta R}{\delta h}
					$$</p>
				<div class="fragment">
				<p>But:</p>
					<ul>
					<li> Requires each neuron measures an IID noise source, $\xi^i$, or knows its output relative to some expected output
					<li> Only well characterized in specific circuits e.g. birdsong learning (Fiete and Seung 2007)</li>
					</ul>
				</div>
				<aside class="notes">
				<span style="color: red"></span> •
				So, given these ideas we can rephrase our question about credit assignment again. We can rephase our problem into a network wanting to know which actions cause reward. And, what is a neuron's causal effect on reward, and so how should it change to improve? 

				Given this interpretation, we can see that methods like those based on REINFORCE address this problem by adding randomization to a neuron's output. That is, you can think of this as a type of RCT approach to doing this causal inference. 

				The question is, can a neuron solve this causal inference problem without randomizing? 
				• <span style="color: green"></span>
			  </aside>
			</section>


			  <section data-background-color="#ffffff">
				  <h2>Causal learning without randomization</h2>
				  An observation: decisions made with arbitary thresholds let us observe counterfactuals
				  <br>
				  <img src="assets/rdd.svg" width="50%">
				  <p class="rcred">Adapted from Moscoe et al, J Clin Epid 2015</p>
					Known as regression discontinuity design (RDD) in economics
				<aside class="notes">
				  <span style="color: red"></span> •
				  We make use of the following observation. Decisions made with arbitary thresholds let us estiamte causal effects. The marginal populations only differ by the fact that one groups recieves the treatment, and the other does not. Thus this removes confounds and lets us measure beta. 
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
			  <section data-background-color="#ffffff">
				<h2>Two more observations:</h2>
					<ol>
						<li> A neuron <em>only</em> spikes if its input is above a threshold
						<li> A spike can have a measurable effect on outcome and reward
					</ol>
		  
					<p class="fragment">Suggests regression discontinuity design can be used by a neuron to estimate its causal effect.<br>		  
				<aside class="notes">
				  <span style="color: red"></span> •
				  This is useful in the neural learning setting. Because a neuron spikes when its input is above a threshold. In some cases at least, a spike does have a measureable effect on a reward signal. 
		  
				  This suggests a neuron could use RDD to estimate its causal effect.

				  Specifically, we will investigate the question, can a neuron use RDD to estimate beta, defined as follows?
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
		  <!-- ---------------------------------------------------------------------- -->
		  <!-- ---------------------------------------------------------------------- -->
		  <!-- ---------------------------------------------------------------------- -->
		  
			  <section data-background-color='#ffffff'>
				<h2>RDD for solving credit assignment</h2>
				  <img src="assets/rdd_fig1a.svg" width="55%">
				  <p class="rcred">Lansdell and Kording, bioRxiv 2019</p>	
					<ul>
						<li> Inputs that place the neuron close to threshold are unbiased estimate of causal effect
						<li> Estimate piece-wise constant model: $$R = \gamma_i + \beta_i H_i$$
					</ul>
		  
				<aside class="notes">
				  <span style="color: red"></span> •
				  So, over a fixed time window, a neuron should only use inputs where it was within p of threshold to learn from. Using this data, we propose a neuron can estimate a piecewise linear model which it can use to infer beta. By design, this works with correlated noise sources -- it only needs to observe the reward and how close it was to spiking, it does not need to identify an IID noise source to estimate causal effects. 
				  • <span style="color: green"></span>
				</aside>
			  </section>

			  <!-- Take a step back and talk about the fact that spiking is a feature, not a bug, here?? -->

			  <section data-background-color='#ffffff'>
				  <h2>A small demonstration</h2>
				  <p>The two-neuron network with noise correlations</p>
				  <img src="assets/fig2a_lif.svg" width="35%">
		  
				<aside class="notes">
				  <span style="color: red"></span> •
				  So how does this look in simulation? We demonstrate the idea on a simple 2 neuron network. We inject two neurons with correlated noise, xi. They have what are called leaky integrate and fire dynamics, which includes a spiking mechanism when the voltage exceeds some value theta. Each neuron runs RDD for a window size p. We can observe, if we fit a piecewise constant model the estimate becomes unbiased for small p. If we use the piece wise linear model, the estimate is in fact unbiased for large value of also. We can compare the RDD estimator to what we called the observed dependence, which takes p = 1. that is we just use all points below and above the threhsold to estimate the relationship. This is confounded for highly correlated inputs. We see on these graphs that bias. 
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  		  
			  <section data-background-color='#ffffff'>
				<h2>A small demonstration</h2>
				<p>Can use RDD to estimate the causal effect</p>
				<img src="assets/fig2a_ace.svg" width="45%">
		
			  <aside class="notes">
				<span style="color: red"></span> •
				So how does this look in simulation? We demonstrate the idea on a simple 2 neuron network. We inject two neurons with correlated noise, xi. They have what are called leaky integrate and fire dynamics, which includes a spiking mechanism when the voltage exceeds some value theta. Each neuron runs RDD for a window size p. We can observe, if we fit a piecewise constant model the estimate becomes unbiased for small p. If we use the piece wise linear model, the estimate is in fact unbiased for large value of also. We can compare the RDD estimator to what we called the observed dependence, which takes p = 1. that is we just use all points below and above the threhsold to estimate the relationship. This is confounded for highly correlated inputs. We see on these graphs that bias. 
				• <span style="color: green"></span>
			  </aside>
			</section>

			<section data-background-color='#ffffff'>
				<h2>A small demonstration</h2>
				<p>In cases where a correlational estimator fails</p>
				$$\beta = \mathbb{E}(R|H\leftarrow 1) - \mathbb{E}(R|H\leftarrow 0), \quad \beta_{OD} = \mathbb{E}(R|H=1) - \mathbb{E}(R|H=0)$$
				<img src="assets/fig2a_lossland.svg" width="85%">
		
			  <aside class="notes">
				<span style="color: red"></span> •
				So how does this look in simulation? We demonstrate the idea on a simple 2 neuron network. We inject two neurons with correlated noise, xi. They have what are called leaky integrate and fire dynamics, which includes a spiking mechanism when the voltage exceeds some value theta. Each neuron runs RDD for a window size p. We can observe, if we fit a piecewise constant model the estimate becomes unbiased for small p. If we use the piece wise linear model, the estimate is in fact unbiased for large value of also. We can compare the RDD estimator to what we called the observed dependence, which takes p = 1. that is we just use all points below and above the threhsold to estimate the relationship. This is confounded for highly correlated inputs. We see on these graphs that bias. 
				• <span style="color: green"></span>
			  </aside>
			</section>
			
			  <section data-background-color='#ffffff'>
				  <h2>A small demonstration</h2>
				  <p>Under some assumptions</p>
				$$
				\frac{\partial R}{\partial w^i_j} \approx \frac{\partial H^i}{\partial w^i_j} \beta^i
				$$
					<ul>
					<li> Can relate causal effect to gradients $\Rightarrow$ derive stochastic gradient descent learning rule 
				  	</ul><br>
				  <img src="assets/fig4.svg" width="100%">
				  <ul>
					<li> Learning trajectories are less biased and converge faster
				  	</ul>
				<aside class="notes">
				  <span style="color: red"></span> •
				  The rule takes the following form. 
		  
				  We observe the trajectories are less biased and converge faster than when using the observed dependence.
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
			<!--  <section data-background-color='#ffffff'>
				<h2>A larger example</h2>
				<div>
					<p>Loss function trains one neuron to have different firing rate from rest of population</p>
				<img src="assets/biofeedback2.svg" width="45%">
				<ul>
					<li> Performance and learning curve independent of noise correlations
					<li> RDD-trained network learns more quickly which neuron is the special
				</ul>
			</div>
		
			  <aside class="notes">
				<span style="color: red"></span> •
		
				• <span style="color: green"></span>
			  </aside>
			</section>

			  <section data-background-color='#ffffff'>
				  <h2>Application to brain-computer interface learning</h2>
				  <div>
					  <ul>
						  <li> In single-unit BCIs, individual neurons are trained through biofeedback </li>
						  <li>Here, causal effect of a neuron is known <em>by construction</em> </li> 
						  <li> How does the network change specifically the control neuron's activity? 
						  <li> Must solve causal inference problem
					  </ul>
				  <img src="assets/biofeedback.svg" width="35%">
				  <p class="rcred">Lansdell et al IEEE Trans NSRE 2020</p>
				  </div>
		  
				<aside class="notes">
				  <span style="color: red"></span> •
				  So, this was for 2 neurons. We can test it in a larger network and more interesting setting than an artifical reward function. 
		  
				  [Go through bullet points]
				  We see with RDD-based learning that performance doesn't matter on the correlation of the control neuron with other wrist-controlling neurons. This is similar to empirical findings, whereas learning with the observed dependence, final performance of the network does depend on the amount of correlation.
		  
				  In training, the weights for the individual control unit are more quickly separated from the rest of the units that are doing wrist control, compared to the observed dependence. 
		  
				  Thus this recapitulates findings from BCI learning.
		  
				  • <span style="color: green"></span>
				</aside>
			  </section>-->

			<!--  <section data-background-color='#ffffff'>
				<h2>Is this plausible?</h2>
					<ul> <li> It would require:
						<ul>
						<li> sub-threshold dependent plasticity
						<li> neuromodulator dependent plasticity</ul>
					</ul>
				<img src="assets/fig5a.svg" width="55%">
				<p class="rcred">Ngezahayo et al 2000, Seol et al 2007</p>
					<!-- <ul class="fragment">
						<li> Additionally would predict super-threshold dependent plasticity
					</ul>
		
			  <aside class="notes">
				<span style="color: red"></span> •
				Of course, we can ask if it is realistic for a neuron to do something like RDD-based learning? In fact, there are two components to the model really. The first is the only learning for inputs that plcae the neuron close to threshold. This is in part already established under something called sub-threshold dependent plasticity. Inputs that place a neuron too far below threhsold induce no learning, consistent with RDD. The model would additionally predict that very high super threshold inputs also induce no learning, but this hasn't been explicitly tested yet. 
		
				The other part of the model is the reward dependent component, in which the sign of the update switches with the magnitude of the reward signal. This has been established in some cases, but more experiments are also needed. Here the blue and red curves show the sign of the changes to weights for different neuromodulator concentrations, showing things can turn from potentiating (increasing) to depressing (decreasing). 
		
				Thus both aspects are largely consistent with neurobiology. 
				• <span style="color: green"></span>
			  </aside>
			</section>-->

			<section>
				<h2>Why spike?</h2>
				<ul>
					<li> Neurons need to communicate over large distances
					</ul>
					<div class = "fragment">
					<video class="slideautostart" src="assets/chris_paper_S1.ogv" muted controls loop autoplay
					poster="assets/CorticalFOV.png" width="50%">
					</video>
					<p class="rcred">Calcium imaging in Hyrda. Dupre and Yuste 2018</p>
					</div>
					<ul class = "fragment">
					<li> Computationally, a spiking discontinuity is inconvenient for learning
					<li> What are the comptuational benefits of spiking? 
					<li class="fragment"> With RDD-based learning, spiking is a feature and not a bug
				</ul>
				<aside class="notes">
				<span style="color: red"></span> •
				The rule takes the following form. 
		
				We observe the trajectories are less biased and converge faster than when using the observed dependence.
				• <span style="color: green"></span>
			  </aside>
			</section>

			  <section data-background="assets/brainbow2.jpg">
				  <h2>Part 1 summary</h2>
				  <ul>
					<li> RDD can be used to estimate causal effects, and can provide a solution to the credit assignment problem in spiking neural networks
					<li> Shows a neuron can do causal inference without needing to randomize
					<li> Relies on the fact that neurons spike when input exceeds a threshold &ndash; spiking is a feature not a bug
				  </ul>
		  
				<aside class="notes">
				  <span style="color: red"></span> •
			  Thus:
			  * neurons can solve the credit assignment problem without an independent noise source, in the presence of high correlations
			  * can learn with only observation of reward and how close it was to spiking
			  * Spiking is a feature, not a bug!
			  * An answer to why spike?
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
		  <!-- ---------------------------------------------------------------------- -->
		  <!-- ---------------------------------------------------------------------- -->
		  <!-- ---------------------------------------------------------------------- -->
		  
			  <section data-background="assets/brainbow2.jpg">
				  <h2>Outline</h2>
				  <ol>
					<li> The neuronal credit assignment problem as causal inference
					<li class="fragment highlight-green"> Learning to solve the credit assignment problem
					<li>Causal learning and decision making</li>
					</ol>
				  <aside class="notes">
				  <span style="color: red"></span> •
				  • <span style="color: green"></span>
			  </aside></section>	  
					  
			  <section data-background-color="#ffffff">
				<h2>Learning rules that scale to larger problems</h2>
				<div>
				<img src="assets/learningrules.png" width="60%">
				<p class="rcred">Richards et al Nature Neuroscience 2019</p>
				<ul>
					<li> Either implicitly or explicitly, many learning rules use gradient information to optimize their weights
					<li> Vary according to their bias and variance
				</ul>
			</div>
				<aside class="notes">
				<span style="color: red"></span> •
				Ok, so we've seen how a neuron can learn without noisily perturbing its activity but instead just noting how close it is to threshold.
				This means it could learn in more general set of circumstances. But it doesn't mean that it can learn as efficiently as something trained with backpropagation. It is still just taking its own activity and relating that to a reward signal. Unlike an algorithm similar to backprop, there is no specific structure that communicates error signals specific to that neuron. 
		
				So, the question we address here is, can such methods be augmented with a network to provide neuron-specific error signals?
				• <span style="color: green"></span>
			  </aside>	</section>	  

			  <section data-background-color="#ffffff">
				<h2>Learning rules that scale to larger problems</h2>
				<div>
				<img src="assets/learningrules.png" width="60%">
				<p class="rcred">Richards et al Nature Neuroscience 2019</p>
				<ul>
					<li> Reinforcement-learning based algorithms do not lead to efficient learning &ndash; high variance estimators
					<li> $\Rightarrow$ Need higher-dimensional error signal to learn from
				  </ul>
			</div>
				<aside class="notes">
				<span style="color: red"></span> •
				Ok, so we've seen how a neuron can learn without noisily perturbing its activity but instead just noting how close it is to threshold.
				This means it could learn in more general set of circumstances. But it doesn't mean that it can learn as efficiently as something trained with backpropagation. It is still just taking its own activity and relating that to a reward signal. Unlike an algorithm similar to backprop, there is no specific structure that communicates error signals specific to that neuron. 
		
				So, the question we address here is, can such methods be augmented with a network to provide neuron-specific error signals?
				• <span style="color: green"></span>
			  </aside>	</section>	  

			  <section data-background-color="#ffffff">
				  <h2>Learning rules that scale to larger problems</h2>
				  <div id = "left">
					  <ul>
						<li>Is this plausible?</li>
						<ul>
						<li> Cortical structure has both feedforward and feedback connections
					  <li> Pyramidal neurons contain both apical and basal compartments, allowing for separate sites of integration (Kording and Konig 2001, Guergiuev et al 2017)
					</ul>
						<li>Does this help?</li>
					</ul>
					$\Rightarrow$ Investigate in the setting of artificial neuron networks
				</div>
				  <div id = "right">
				  <img src="assets/cortex_schematic.png" width="80%">
				  </div>
				  <aside class="notes">
				  <span style="color: red"></span> •
				  Ok, so we've seen how a neuron can learn without noisily perturbing its activity but instead just noting how close it is to threshold.
				  This means it could learn in more general set of circumstances. But it doesn't mean that it can learn as efficiently as something trained with backpropagation. It is still just taking its own activity and relating that to a reward signal. Unlike an algorithm similar to backprop, there is no specific structure that communicates error signals specific to that neuron. 
		  
				  So, the question we address here is, can such methods be augmented with a network to provide neuron-specific error signals?
				  • <span style="color: green"></span>
				</aside>	</section>	  
		  
				<section data-background-color="#ffffff">
					<h2>Biologically implausible backpropagation</h2>
					<div id = "left">
					<ul>
						<li> The gradient algorithm, backpropagation, suggests one form of such a network
						<li> Low bias, low variance &ndash; 'the workhorse of deep learning' 
						<li> But weights of this network would be transpose of the feedforward weights &ndash; e.g. require weight transport
					</ul>
					</div>
					<div id = "right">
					<img src="assets/fig1_schematic_bp.svg" width="63%">
					</div>
					<aside class="notes">
					<span style="color: red"></span> •
						Backpropagation, as mentioned, suggests one form of this feedback networks. But also as mentioned, this has known problems, notably the so-called weight transport problem. 

					• <span style="color: green"></span>
				  </aside>	</section>	  
		  
			  <section data-background-color="#ffffff">
				  <h2>Learning without weight transport</h2>
				  <div id = "left">
					  However:
				 <ul>
					<li>Weight transport can be avoided by using random, fixed feedback weights, $B_i$ &ndash; feedback alignment
					<li> Works on small fully-connected networks
						<li> Suggests high-dimensional error signals help, even if very biased
						</ul>
					But:
					<ul>
						<li> Doesn't work on deep networks, CNNs, networks with bottleneck layers
				  </ul>
				  <p class="fragment">$\Rightarrow$ Can we improve on feedback alignment by learning weights $B_i$?</p>
					</div>
				  <div id = "right">
				  <img src="assets/fig1_schematic_fa.svg" width="60%">
				  </div>
				  <aside class="notes">
				  <span style="color: red"></span> •
				  The problem with current models is that they work around this weight transport problem by using just random, fixed weights. This works on small networks, through a phenomenon called feedback alignment. 
				  • <span style="color: green"></span>
				</aside>	</section>
		  
			  <section data-background-color="#ffffff">
				  <h2>Learning feedback weights with perturbations</h2>
				  <div id = "left">
				  Minimize $\mathbb{E}(\mathcal{L}(\mathbf{x}, \mathbf{y}))$. Assume activations are noisy: 
				  $$
				  \mathbf{h}_t^i = \sigma\left(\sum_k W^i_{\cdot k} \mathbf{h}_t^{i-1}\right) + c_h\xi^i_t
				  $$ 
				  REINFORCE-type estimator of error gradient:
				  \begin{equation*}
				  \hat{\lambda}^i = (\tilde{\mathcal{L}}(\mathbf{x},\mathbf{y},\xi)-\mathcal{L}(\mathbf{x},\mathbf{y})) \frac{\xi^i}{c_h} \approx \frac{\partial \mathcal{L}}{\partial \mathbf{h}^i}
				  \end{equation*}
				  Train feedback network to provide a useful error signal:
				  \begin{equation}
				  \label{eq:lsq}
				  \hat{B}^{i+1} = \text{argmin}_{B} \mathbb{E}\left\| B^T\mathbf{e}^{i+1} - \hat{\lambda^i} \right\|_2^2
				  \end{equation}
				  </div>
				  <div id = "right">			
				  <img src="assets/fig1_schematic_np.svg" width="85.5%">
				  </div>
				  <aside class="notes">
				  <span style="color: red"></span> •
				  • <span style="color: green"></span>
				</aside>	</section>

				  <section data-background-color="#ffffff">
				  <h2>Learning feedback weights with perturbations</h2>
				  
					<p>Weights in the final layer converge, in the following way</p>
				  <p><em>Theorem 1:</em> The least squares estimator
		  \begin{equation*}
		  (\hat{B}^{N+1})^T = \hat{\lambda}^N (\mathbf{e}^{N+1})^T\left(\mathbf{e}^{N+1}(\mathbf{e}^{N+1})^T\right)^{-1},
		  \end{equation*}
		  converges to the true feedback matrix, in the sense that:
		  $$
		  \lim_{c_h\to 0}\text{plim}_{T\to\infty} \hat{B}^{N+1} = W^{N+1},
		  $$
		  where $\text{plim}$ indicates convergence in probability. </p>
		  <br>
				  <aside class="notes">
				  <span style="color: red"></span> •
				  • <span style="color: green"></span>
				</aside>	</section>

				<section data-background-color="#ffffff">
					<h2>Learning feedback weights with perturbations</h2>

					<p>Weights in all layers converge, for a linear network</p>
			<p><em>Theorem 2:</em> For $\sigma(x) = x$, the least squares estimator
				$$
			\begin{equation*}
			(\hat{B}^{n})^T = \hat{\lambda}^{n-1} (\mathbf{\tilde{e}}^{n})^T\left(\mathbf{\tilde{e}}^{n}(\mathbf{\tilde{e}}^{n})^T\right)^{-1}\qquad 1 \le n \le N+1, 
			\end{equation*}$$
			converges to the true feedback matrix, in the sense that:
			$$
			\lim_{c_h\to 0}\text{plim}_{T\to\infty} \hat{B}^{n} = W^{n}, \qquad 1 \le n \le N+1.
			$$
			</p>
					<aside class="notes">
					<span style="color: red"></span> •
					• <span style="color: green"></span>
				  </aside>	</section>

				  <section data-background-color="#ffffff">
				  <h2>A small example</h2>
				  <ul>
					  <li> Test on a 4 layer network solving MNIST
					  <li> Learns to more closely approximate true gradient than random weights
				  </ul>
				  <img src="assets/fig_2b_validate_nodepert_varw_correctbatches.png" width="90%">
				  <p class="rcred">Lansdell, Prakash and Kording, ICLR 2020</p>
				  <aside class="notes">
				  <span style="color: red"></span> •
			  A 4 layer network solving MNIST
				  • <span style="color: green"></span>
				</aside>	</section>
		  
			  <section data-background-color="#ffffff">
				  <h2>A (slightly) larger example</h2>
				  <ul>
					  <li> Test on a 5 layer autoencoding network on MNIST
					  <li> Feedback alignment fails to solve this task
					  <li> Node perturbation learns <em>faster</em> than backprop w stochastic gradient descent
					  <li> Comparable to BP with ADAM optimzer
				  </ul>
				  <img src="assets/3_autoencoder.png" width="95%">
					  <aside class="notes">
				  <span style="color: red"></span> •
			  An autoencoding network: outperforms BP, and performs similiarly to ADAM
				  • <span style="color: green"></span>
				</aside>	</section>
		  		  
				<section data-background-color="#ffffff">
					<h2>A larger example</h2>
					<ul>
						<li>Also leads to improved performance on CNNs
						<li>(Too deep to propagate approximate signals through all layers
						<br>$\Rightarrow$ Use <em>direct feedback alignment</em> instead)
					</ul>
					<br><br>
					<table style="width:100%">
						<tr>
						<th>dataset</th>
						<th>BP</th>
						<th>NP</th>
						<th>DFA</th>
						</tr>
						<tr>
						<td>CIFAR10</td>
						<td>76.9$\pm$0.1</td>
						<td>74.8$\pm$0.2</td>
						<td>72.4$\pm$0.2</td>
						</tr>
						<tr>
						<td>CIFAR100</td>
						<td>51.2$\pm$0.1</td>
						<td>48.1$\pm$0.2</td>
						<td>47.3$\pm$0.1</td>
						</tr>
					</table> 
					Mean test accuracy of CNN over 5 runs trained with backpropagation, node perturbation and direct feedback alignment (DFA)
					<br><br>
					<p class="fragment">$\Rightarrow$ Shows challenging computer vision problems can be solved without weight transport</p>

						<aside class="notes">
					<span style="color: red"></span> •
				An autoencoding network: outperforms BP, and performs similiarly to ADAM
					• <span style="color: green"></span>
				  </aside>	</section>

				  <section data-background="assets/brainbow2.jpg">
				  <h2>Summary</h2>
				  <ul>
					  <li> Shown how:
					  <ul><li> neurons can use their spiking threshold to estimate their causal effect on reward
					  <li> a perturbation-based learning rule can be used to train a feedback network to provide useful error information
					  </ul>
					  <li>Applications in:
						  <ul>
							  <li>Neuromorphic hardware &ndash; learning with spiking networks</li>
							  <li>Application specific integrated circuits (ASICs) &ndash; learning without weight transport</li>
						  </ul>
					  </li>
					  <li> A combination of these can provide biologically plausible and scaleable learning systems
					</ul>
					  <aside class="notes">
				  <span style="color: red"></span> •
					   We believe a combination of these two approaches: credit assignment through causal inference, and learning a feedback system can provide a plausible and scalable learning system that could be implemented in the brain.........
				  • <span style="color: green"></span>
				</aside>	</section>

				<section data-background="assets/brainbow2.jpg">
					<h2>Outline</h2>
					<ol>
						<li> The neuronal credit assignment problem as causal inference
						<li> Learning to solve the credit assignment problem
						<li class="fragment highlight-green">Causal learning and decision making</li>
					</ol>
						<aside class="notes">
					<span style="color: red"></span> •
					* For the bulk of this talk, the aim is to see how that plays out in one particular example in detail, in particular in a problem called the credit assignment problem
					• <span style="color: green"></span>
				  </aside>	</section>	  

				<section data-background-color="#ffffff">
					<h2>Causal learning and decision making</h2>
					<div>
					What is the effect of my actions, and how do I use this to maximize reward?
					<!--Something of a combination between CI and RL. This requires thinking about causality in cases where we have limited ability to interact with or make changes in the world — we must combine what we learn from observation, taking actions to maximize reward, and taking actions to learn about the world.-->
					</div>
					<br>
					If actions/experimentation is cheap/easy/etc<br><br>
					<img src="assets/causalrl.svg" width="45%">
						<aside class="notes">
					<span style="color: red"></span> •
					• <span style="color: green"></span>
				  </aside>	</section>

				  <section data-background-color="#ffffff">
					<h2>Causal learning and decision making</h2>
					<div>
					What is the effect of my actions, and how do I use this to maximize reward?
					</div><br>
					If actions/experimentation is difficult/expensive/etc<br><br>
					<img src="assets/causalrl_ol.svg" width="45%"><br><br>
					<br>~Reinforcement learning + causal inference<br><br>
					<aside class="notes">
					<span style="color: red"></span> •
					• <span style="color: green"></span>
				  </aside>	</section>
				  
				  <section data-background-color="#ffffff">
					<h2>Causal learning and decision making</h2>
					<div>
					What is the effect of my actions, and how do I use this to maximize reward?
					</div>
					<div id = "left"><br>
					<ol>
					<li style="color: red">Neuronal credit assignment problem</li>
					<li>Conservative exploration</li>
					<li>Combining observational learning with reinforcement learning</li>
					</ol>
					</div>
					<div id = "right"><br>
						<h5>BCIs as a testbed for credit assignment</h5>
						<img src="assets/dualbci.png" width="85%">
						<p class="rcred">Lansdell et al <em>IEEE Trans NSRE</em> 2020<br>
							Aljadeff et al <em>Neuron</em> 2016<br>
							Lansdell et al <em>PLoS Comp Biol.</em> 2014
						</p>
					</div>
					<aside class="notes">
					<span style="color: red"></span> •
					• <span style="color: green"></span>
				  </aside>	</section>
				  
				  <section data-background-color="#ffffff">
					<h2>Causal learning and decision making</h2>
					<div>
					What is the effect of my actions, and how do I use this to maximize reward?
					</div>
					<div id = "left">
					<br>
					<ol>
					<li>Neuronal credit assignment problem</li>
					<li style = "color: red">Conservative exploration</li>
					<li>Combining observational learning with reinforcement learning</li>
					</ol>
					</div>
					<div id = "right"><br>
						<h5>Thresholding policies allow conservative exploration in contextual multi-armed bandits</h5>
						<img src="assets/bandits_intro_v2.svg" width="75%">
						<p class="rcred">Lansdell, Triantafillou, and Kording 2020. <em>Submitted</em></p>
					</div>
					<aside class="notes">
					<span style="color: red"></span> •
					• <span style="color: green"></span>
				  </aside>	</section>

				<section data-background-color="#ffffff">
					<h2>Causal learning and decision making</h2>
					<div>
					What is the effect of my actions, and how do I use this to maximize reward?
					</div>
					<div id = "left"><br>
					<ol>
					<li>Neuronal credit assignment problem</li>
					<li>Conservative exploration</li>
					<li style = "color: red">Combining observational learning with reinforcement learning</li>
					</ol>
					</div>
					<div id = "right">
						<br>
						<h5>Meta-RL for observational causal learning</h5>
						<p class="rcred">Lansdell <em>In preparation</em><br>
						Lansdell, Kording. Curr. Opin. Behav. Sci. 2019</p>
						<img src="assets/metarl_buttons.gif" width="40%">
					</div>
					<div style="clear: both">
					<p style="text-align: center"></p>
					</div>
					<aside class="notes">
					<span style="color: red"></span> •
					• <span style="color: green"></span>
				  </aside>	</section>

				  <section data-background-color="#ffffff">
					<h2>Causal learning and decision making</h2>
					<div>
					What is the effect of my actions, and how do I use this to maximize reward?
					</div>
					<div id = "left"><br>
					<ol>
					<li>Neuronal credit assignment problem</li>
					<li>Conservative exploration</li>
					<li>Combining observational learning with reinforcement learning</li>
					</ol>
					</div>
					<div style="clear: both"><br>
					<p style="text-align: center">Causal models can improve both ML and models of our own learning</p>
					</div>
					<aside class="notes">
					<span style="color: red"></span> •
					• <span style="color: green"></span>
				  </aside>	</section>

	<section data-background-image="./assets/brainbow2.jpg">
		<h2>References</h2>
			<hr>
		<ul style="font-size: smaller">
			<li> <b>Lansdell B</b>, Kording K, "Spiking allows neurons to estimate their causal effect" biorxiv 2019
			<li> <b>Lansdell B</b>, Triantafilou S, Kording K "Rarely-switching linear bandits: causal effects optimization in the real world" arxiv 2019
			<li> <b>Lansdell B</b>, Prakash P, Kording K, "Learning to solving the credit assignment problem" ICLR 2020, Addis Ababa, Ethiopia
			<li> <b>Lansdell B</b>, Milovanovic I, Mellema C, Fairhall A, Fetz E, Moritz C, "Reconfiguring motor circuits for a joint manual and BCI task" IEEE Trans. Neural Systems and Rehabilitation Engineering, 2020, 28(1)
 			<li><b>Lansdell B</b>, Kording K, "Towards learning-to-learn" Current Opinion in Behavioral Science, 2019, 29, 45-50
			<li>Aljadeff Y, <b>Lansdell B</b>, Fairhall A, Kleinfeld D, "Analysis of neuronal spike trains, deconstructed", Neuron 2016, 91(2)
			<li>Pang R, <b>Lansdell B</b>, Fairhall A, "Dimensionality Reduction in Neuroscience", Current Biology 2016, 26: R1-R5
			<li><b>Lansdell B</b>, Ford K, Kutz J N, "A reaction-diffusion model of cholinergic retinal waves", PLoS Computational Biology, 2014, 10(12): e1003953
		</ul>
			<aside class="notes">
		  <span style="color: red"></span> •
		  • <span style="color: green"></span>
		</aside>
	  </section>

			  <section data-background-image="./assets/brainbow2.jpg">
				<h2>Acknowledgments</h2>
					<hr>
				<div id="left">
			   <ul>
					<li> Konrad Kording (U Penn)
				   <li> Kording lab</li><ul>
					   <li> Ari Benjamin </li>
					   <li> David Rolnick</li>
					   <li> Roozbeh Farhoodi</li>
					   <li> Prashanth Prakash</li></ul>
					<li> Adrienne Fairhall (UW)</li>
					<li> Fairhall lab</li><ul>
						<li> Rich Pang</li>
						<li> Alison Duffy</li>
					</ul>
				</ul>
					</div>
					<div id="right">
					<ul>
					<li> Chet Moritz (UW) </li>
					<li> Ivana Milovanovic (UW)</li>
					<li> Cooper Mellema (UT Austin)</li>
					<li> Eberhard Fetz (UW)</li>
					<li> Sofia Triantafilou (UPitt) </li>
					<li> J Nathan Kutz (UW)</li>
					<li> Kevin Ford (UC Berkeley)</li>
					<li> David Kleinfeld (UCSD)</li>
					<li> Yonatan Aljadeff (ICL)</li>
					</ul>
						</div>
					<aside class="notes">
				  <span style="color: red"></span> •
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
		  <!-- ----------------------------------------------------------------------- -->
		  <!-- ----------------------------------------------------------------------- -->
		  <!-- ----------------------------------------------------------------------- -->
		  
			  <section data-background-color='#ffffff'>
				<h2>RDD as a way for a neuron to solve credit assignment</h2>
				  <img src="assets/rdd_fig1b.svg" width="80%">
				  <p class="rcred">Lansdell and Kording, bioRxiv 2019</p>
		  
				<aside class="notes">
				  <span style="color: red"></span> •
				  The idea is that it should only estimate the effect on a reward signal for inputs that place the neuron close to threshold. Over a fixed time window, we have a set of below-threshold inputs, marginally below, marginally above, and above. The idea is that neuron's should only update their estimate of beta for the marginal inputs, and that is a way of isolating that neuron's specific effect on reward. 
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
			  <section data-background-color='#ffffff'>
				  <h2>Is this plausible?</h2>
					  <ul> <li> Consistent with:
						  <ul>
						  <li> current models of sub-threshold dependent plasticity
						  <li> current models of neuromodulator dependent plasticity</ul>
					  </ul>
				  <img src="assets/fig5a.svg" width="55%">
					  <!-- <ul class="fragment">
						  <li> Additionally would predict super-threshold dependent plasticity
					  </ul>-->
		  
				<aside class="notes">
				  <span style="color: red"></span> •
				  Of course, we can ask if it is realistic for a neuron to do something like RDD-based learning? In fact, there are two components to the model really. The first is the only learning for inputs that plcae the neuron close to threshold. This is in part already established under something called sub-threshold dependent plasticity. Inputs that place a neuron too far below threhsold induce no learning, consistent with RDD. The model would additionally predict that very high super threshold inputs also induce no learning, but this hasn't been explicitly tested yet. 
		  
				  The other part of the model is the reward dependent component, in which the sign of the update switches with the magnitude of the reward signal. This has been established in some cases, but more experiments are also needed. Here the blue and red curves show the sign of the changes to weights for different neuromodulator concentrations, showing things can turn from potentiating (increasing) to depressing (decreasing). 
		  
				  Thus both aspects are largely consistent with neurobiology. 
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
			  <section data-background-color='#ffffff'>
				  <h2>How to test?</h2>
					  <ul>
						  <li> Over a fixed time window a reward is administered when neuron spikes
						  <li> Stimuli are identified which place the neuron's input drive close to spiking threshold. 
						  <li> RDD-based learning predicts an increase synaptic changes for a set of stimuli containing a high proportion of near threshold inputs, but that keeps overall firing rate constant.
					  </ul>
				  <img src="assets/fig5b.svg" width="55%">
		  
				<aside class="notes">
				  <span style="color: red"></span> •
				  How would we test RDD-based learning? One idea would be [idea]
				  • <span style="color: green"></span>
				</aside>
			  </section>
			</div>
		</div>

		<script src="js/reveal.js"></script>
		<script src="lib/js/head.min.js"></script>
		<script>
			head.js(
			"lib/js/jquery.min.js",
			"lib/js/jquery.hotkeys.js",
			"lib/js/underscore.min.js",
			"lib/js/swfobject.js",
			"lib/js/dat.gui.js",
			"lib/js/EventEmitter.js",

			function() {
			Reveal.initialize({
			controls: false,
			progress: true,
			history: true,
			center: false,
			keyboard: true,
			touch: false,
			overview: true,
			mouseWheel: false,
			width: 960,
			height: 720,

			theme: false, // hardcoded with CSS import in <head>
			transition: 'fade', // default/cube/page/concave/zoom/linear/fade/none
			transitionSpeed: 'default', // default/fast/slow

			math: {
				mathjax: 'mathjax/MathJax.js',
				config: 'TeX-AMS_HTML-full',
			},

			dependencies: [
				{ src: 'reveal.js/lib/js/classList.js',
			condition: function() { return !document.body.classList; }},
				{ src: 'reveal.js/plugin/markdown/marked.js',
			condition: function() { return !!document.querySelector ('[data-markdown]'); }},
				{ src: 'reveal.js/plugin/markdown/markdown.js',
			condition: function() { return !!document.querySelector ('[data-markdown]'); }},
				{ src: 'reveal.js/plugin/highlight/highlight.js', async: true,
			callback: function() { hljs.initHighlightingOnLoad (); }},
				{ src: 'reveal.js/plugin/notes/notes.js', async: true,
			condition: function() { return !!document.body.classList; }},
				{ src: 'mymath.js', async: true },
			//{ src: 'pdfimgs.js', async: true },
			{ src: 'slideautostart.js', async: true },
			],
			});
		});
  </script>
	</body>
</html>

Ok, so those are the two projects I want to go into in some detail. But now I want to take a step back, and look at how these projects fit into a broader picture, and outline future work. So these projects fit into a general topic which is combinations of causal inference and decision making. 